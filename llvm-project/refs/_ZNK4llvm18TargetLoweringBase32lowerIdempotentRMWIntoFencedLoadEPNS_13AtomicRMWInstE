<def f='llvm/llvm/include/llvm/CodeGen/TargetLowering.h' l='1837' ll='1840' type='llvm::LoadInst * llvm::TargetLoweringBase::lowerIdempotentRMWIntoFencedLoad(llvm::AtomicRMWInst * RMWI) const'/>
<doc f='llvm/llvm/include/llvm/CodeGen/TargetLowering.h' l='1826'>/// On some platforms, an AtomicRMW that never actually modifies the value
  /// (such as fetch_add of 0) can be turned into a fence followed by an
  /// atomic load. This may sound useless, but it makes it possible for the
  /// processor to keep the cacheline shared, dramatically improving
  /// performance. And such idempotent RMWs are useful for implementing some
  /// kinds of locks, see for example (justification + benchmarks):
  /// http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf
  /// This method tries doing that transformation, returning the atomic load if
  /// it succeeds, and nullptr otherwise.
  /// If shouldExpandAtomicLoadInIR returns true on that load, it will undergo
  /// another round of expansion.</doc>
<use f='llvm/llvm/lib/CodeGen/AtomicExpandPass.cpp' l='1337' u='c' c='_ZN12_GLOBAL__N_112AtomicExpand21simplifyIdempotentRMWEPN4llvm13AtomicRMWInstE'/>
<ovr f='llvm/llvm/lib/Target/X86/X86ISelLowering.cpp' l='25865' c='_ZNK4llvm17X86TargetLowering32lowerIdempotentRMWIntoFencedLoadEPNS_13AtomicRMWInstE'/>
