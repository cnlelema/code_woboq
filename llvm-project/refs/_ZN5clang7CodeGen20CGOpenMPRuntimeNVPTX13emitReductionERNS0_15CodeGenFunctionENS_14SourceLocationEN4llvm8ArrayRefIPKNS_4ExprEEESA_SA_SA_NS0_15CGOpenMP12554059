<dec f='llvm/clang/lib/CodeGen/CGOpenMPRuntimeNVPTX.h' l='313' type='void clang::CodeGen::CGOpenMPRuntimeNVPTX::emitReduction(clang::CodeGen::CodeGenFunction &amp; CGF, clang::SourceLocation Loc, ArrayRef&lt;const clang::Expr *&gt; Privates, ArrayRef&lt;const clang::Expr *&gt; LHSExprs, ArrayRef&lt;const clang::Expr *&gt; RHSExprs, ArrayRef&lt;const clang::Expr *&gt; ReductionOps, clang::CodeGen::CGOpenMPRuntime::ReductionOptionsTy Options)'/>
<inh f='llvm/clang/lib/CodeGen/CGOpenMPRuntime.h' l='1309' c='_ZN5clang7CodeGen15CGOpenMPRuntime13emitReductionERNS0_15CodeGenFunctionENS_14SourceLocationEN4llvm8ArrayRefIPKNS_4ExprEEESA_SA_SA_NS1_18ReductionOptionsTyE'/>
<def f='llvm/clang/lib/CodeGen/CGOpenMPRuntimeNVPTX.cpp' l='4203' ll='4386' type='void clang::CodeGen::CGOpenMPRuntimeNVPTX::emitReduction(clang::CodeGen::CodeGenFunction &amp; CGF, clang::SourceLocation Loc, ArrayRef&lt;const clang::Expr *&gt; Privates, ArrayRef&lt;const clang::Expr *&gt; LHSExprs, ArrayRef&lt;const clang::Expr *&gt; RHSExprs, ArrayRef&lt;const clang::Expr *&gt; ReductionOps, clang::CodeGen::CGOpenMPRuntime::ReductionOptionsTy Options)'/>
<doc f='llvm/clang/lib/CodeGen/CGOpenMPRuntimeNVPTX.cpp' l='3961'>///
/// Design of OpenMP reductions on the GPU
///
/// Consider a typical OpenMP program with one or more reduction
/// clauses:
///
/// float foo;
/// double bar;
/// #pragma omp target teams distribute parallel for \
///             reduction(+:foo) reduction(*:bar)
/// for (int i = 0; i &lt; N; i++) {
///   foo += A[i]; bar *= B[i];
/// }
///
/// where &apos;foo&apos; and &apos;bar&apos; are reduced across all OpenMP threads in
/// all teams.  In our OpenMP implementation on the NVPTX device an
/// OpenMP team is mapped to a CUDA threadblock and OpenMP threads
/// within a team are mapped to CUDA threads within a threadblock.
/// Our goal is to efficiently aggregate values across all OpenMP
/// threads such that:
///
///   - the compiler and runtime are logically concise, and
///   - the reduction is performed efficiently in a hierarchical
///     manner as follows: within OpenMP threads in the same warp,
///     across warps in a threadblock, and finally across teams on
///     the NVPTX device.
///
/// Introduction to Decoupling
///
/// We would like to decouple the compiler and the runtime so that the
/// latter is ignorant of the reduction variables (number, data types)
/// and the reduction operators.  This allows a simpler interface
/// and implementation while still attaining good performance.
///
/// Pseudocode for the aforementioned OpenMP program generated by the
/// compiler is as follows:
///
/// 1. Create private copies of reduction variables on each OpenMP
///    thread: &apos;foo_private&apos;, &apos;bar_private&apos;
/// 2. Each OpenMP thread reduces the chunk of &apos;A&apos; and &apos;B&apos; assigned
///    to it and writes the result in &apos;foo_private&apos; and &apos;bar_private&apos;
///    respectively.
/// 3. Call the OpenMP runtime on the GPU to reduce within a team
///    and store the result on the team master:
///
///     __kmpc_nvptx_parallel_reduce_nowait_v2(...,
///        reduceData, shuffleReduceFn, interWarpCpyFn)
///
///     where:
///       struct ReduceData {
///         double *foo;
///         double *bar;
///       } reduceData
///       reduceData.foo = &amp;foo_private
///       reduceData.bar = &amp;bar_private
///
///     &apos;shuffleReduceFn&apos; and &apos;interWarpCpyFn&apos; are pointers to two
///     auxiliary functions generated by the compiler that operate on
///     variables of type &apos;ReduceData&apos;.  They aid the runtime perform
///     algorithmic steps in a data agnostic manner.
///
///     &apos;shuffleReduceFn&apos; is a pointer to a function that reduces data
///     of type &apos;ReduceData&apos; across two OpenMP threads (lanes) in the
///     same warp.  It takes the following arguments as input:
///
///     a. variable of type &apos;ReduceData&apos; on the calling lane,
///     b. its lane_id,
///     c. an offset relative to the current lane_id to generate a
///        remote_lane_id.  The remote lane contains the second
///        variable of type &apos;ReduceData&apos; that is to be reduced.
///     d. an algorithm version parameter determining which reduction
///        algorithm to use.
///
///     &apos;shuffleReduceFn&apos; retrieves data from the remote lane using
///     efficient GPU shuffle intrinsics and reduces, using the
///     algorithm specified by the 4th parameter, the two operands
///     element-wise.  The result is written to the first operand.
///
///     Different reduction algorithms are implemented in different
///     runtime functions, all calling &apos;shuffleReduceFn&apos; to perform
///     the essential reduction step.  Therefore, based on the 4th
///     parameter, this function behaves slightly differently to
///     cooperate with the runtime to ensure correctness under
///     different circumstances.
///
///     &apos;InterWarpCpyFn&apos; is a pointer to a function that transfers
///     reduced variables across warps.  It tunnels, through CUDA
///     shared memory, the thread-private data of type &apos;ReduceData&apos;
///     from lane 0 of each warp to a lane in the first warp.
/// 4. Call the OpenMP runtime on the GPU to reduce across teams.
///    The last team writes the global reduced value to memory.
///
///     ret = __kmpc_nvptx_teams_reduce_nowait(...,
///             reduceData, shuffleReduceFn, interWarpCpyFn,
///             scratchpadCopyFn, loadAndReduceFn)
///
///     &apos;scratchpadCopyFn&apos; is a helper that stores reduced
///     data from the team master to a scratchpad array in
///     global memory.
///
///     &apos;loadAndReduceFn&apos; is a helper that loads data from
///     the scratchpad array and reduces it with the input
///     operand.
///
///     These compiler generated functions hide address
///     calculation and alignment information from the runtime.
/// 5. if ret == 1:
///     The team master of the last team stores the reduced
///     result to the globals in memory.
///     foo += reduceData.foo; bar *= reduceData.bar
///
///
/// Warp Reduction Algorithms
///
/// On the warp level, we have three algorithms implemented in the
/// OpenMP runtime depending on the number of active lanes:
///
/// Full Warp Reduction
///
/// The reduce algorithm within a warp where all lanes are active
/// is implemented in the runtime as follows:
///
/// full_warp_reduce(void *reduce_data,
///                  kmp_ShuffleReductFctPtr ShuffleReduceFn) {
///   for (int offset = WARPSIZE/2; offset &gt; 0; offset /= 2)
///     ShuffleReduceFn(reduce_data, 0, offset, 0);
/// }
///
/// The algorithm completes in log(2, WARPSIZE) steps.
///
/// &apos;ShuffleReduceFn&apos; is used here with lane_id set to 0 because it is
/// not used therefore we save instructions by not retrieving lane_id
/// from the corresponding special registers.  The 4th parameter, which
/// represents the version of the algorithm being used, is set to 0 to
/// signify full warp reduction.
///
/// In this version, &apos;ShuffleReduceFn&apos; behaves, per element, as follows:
///
/// #reduce_elem refers to an element in the local lane&apos;s data structure
/// #remote_elem is retrieved from a remote lane
/// remote_elem = shuffle_down(reduce_elem, offset, WARPSIZE);
/// reduce_elem = reduce_elem REDUCE_OP remote_elem;
///
/// Contiguous Partial Warp Reduction
///
/// This reduce algorithm is used within a warp where only the first
/// &apos;n&apos; (n &lt;= WARPSIZE) lanes are active.  It is typically used when the
/// number of OpenMP threads in a parallel region is not a multiple of
/// WARPSIZE.  The algorithm is implemented in the runtime as follows:
///
/// void
/// contiguous_partial_reduce(void *reduce_data,
///                           kmp_ShuffleReductFctPtr ShuffleReduceFn,
///                           int size, int lane_id) {
///   int curr_size;
///   int offset;
///   curr_size = size;
///   mask = curr_size/2;
///   while (offset&gt;0) {
///     ShuffleReduceFn(reduce_data, lane_id, offset, 1);
///     curr_size = (curr_size+1)/2;
///     offset = curr_size/2;
///   }
/// }
///
/// In this version, &apos;ShuffleReduceFn&apos; behaves, per element, as follows:
///
/// remote_elem = shuffle_down(reduce_elem, offset, WARPSIZE);
/// if (lane_id &lt; offset)
///     reduce_elem = reduce_elem REDUCE_OP remote_elem
/// else
///     reduce_elem = remote_elem
///
/// This algorithm assumes that the data to be reduced are located in a
/// contiguous subset of lanes starting from the first.  When there is
/// an odd number of active lanes, the data in the last lane is not
/// aggregated with any other lane&apos;s dat but is instead copied over.
///
/// Dispersed Partial Warp Reduction
///
/// This algorithm is used within a warp when any discontiguous subset of
/// lanes are active.  It is used to implement the reduction operation
/// across lanes in an OpenMP simd region or in a nested parallel region.
///
/// void
/// dispersed_partial_reduce(void *reduce_data,
///                          kmp_ShuffleReductFctPtr ShuffleReduceFn) {
///   int size, remote_id;
///   int logical_lane_id = number_of_active_lanes_before_me() * 2;
///   do {
///       remote_id = next_active_lane_id_right_after_me();
///       # the above function returns 0 of no active lane
///       # is present right after the current lane.
///       size = number_of_active_lanes_in_this_warp();
///       logical_lane_id /= 2;
///       ShuffleReduceFn(reduce_data, logical_lane_id,
///                       remote_id-1-threadIdx.x, 2);
///   } while (logical_lane_id % 2 == 0 &amp;&amp; size &gt; 1);
/// }
///
/// There is no assumption made about the initial state of the reduction.
/// Any number of lanes (&gt;=1) could be active at any position.  The reduction
/// result is returned in the first active lane.
///
/// In this version, &apos;ShuffleReduceFn&apos; behaves, per element, as follows:
///
/// remote_elem = shuffle_down(reduce_elem, offset, WARPSIZE);
/// if (lane_id % 2 == 0 &amp;&amp; offset &gt; 0)
///     reduce_elem = reduce_elem REDUCE_OP remote_elem
/// else
///     reduce_elem = remote_elem
///
///
/// Intra-Team Reduction
///
/// This function, as implemented in the runtime call
/// &apos;__kmpc_nvptx_parallel_reduce_nowait_v2&apos;, aggregates data across OpenMP
/// threads in a team.  It first reduces within a warp using the
/// aforementioned algorithms.  We then proceed to gather all such
/// reduced values at the first warp.
///
/// The runtime makes use of the function &apos;InterWarpCpyFn&apos;, which copies
/// data from each of the &quot;warp master&quot; (zeroth lane of each warp, where
/// warp-reduced data is held) to the zeroth warp.  This step reduces (in
/// a mathematical sense) the problem of reduction across warp masters in
/// a block to the problem of warp reduction.
///
///
/// Inter-Team Reduction
///
/// Once a team has reduced its data to a single value, it is stored in
/// a global scratchpad array.  Since each team has a distinct slot, this
/// can be done without locking.
///
/// The last team to write to the scratchpad array proceeds to reduce the
/// scratchpad array.  One or more workers in the last team use the helper
/// &apos;loadAndReduceDataFn&apos; to load and reduce values from the array, i.e.,
/// the k&apos;th worker reduces every k&apos;th element.
///
/// Finally, a call is made to &apos;__kmpc_nvptx_parallel_reduce_nowait_v2&apos; to
/// reduce across workers and compute a globally reduced value.
///</doc>
<doc f='llvm/clang/lib/CodeGen/CGOpenMPRuntimeNVPTX.h' l='300'>/// Emit a code for reduction clause.
  ///
  /// \param Privates List of private copies for original reduction arguments.
  /// \param LHSExprs List of LHS in \a ReductionOps reduction operations.
  /// \param RHSExprs List of RHS in \a ReductionOps reduction operations.
  /// \param ReductionOps List of reduction operations in form &apos;LHS binop RHS&apos;
  /// or &apos;operator binop(LHS, RHS)&apos;.
  /// \param Options List of options for reduction codegen:
  ///     WithNowait true if parent directive has also nowait clause, false
  ///     otherwise.
  ///     SimpleReduction Emit reduction operation only. Used for omp simd
  ///     directive on the host.
  ///     ReductionKind The kind of reduction to perform.</doc>
